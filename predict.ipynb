{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38705a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Excelファイルを読み込む\n",
    "df = pd.read_excel(r\"C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\ver1_20250729.xlsx\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ccf720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# --- 1. データ準備 ---\n",
    "df = pd.read_excel(r\"C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\ver1_20250729.xlsx\")\n",
    "idx_event = df[['INDEX', 'Event']]\n",
    "df_values = df.drop(columns=['INDEX','Event','付属_1','PI','Hard_endpoint'])\n",
    "\n",
    "# --- 2. LGBMRegressor を推定器に指定 ---\n",
    "est_lgb = LGBMRegressor(\n",
    "    n_estimators=50,      # 木の本数\n",
    "    learning_rate=0.1,    # 学習率\n",
    "    n_jobs=-1,            # 全コア並列\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "imp = IterativeImputer(\n",
    "    estimator=est_lgb,\n",
    "    max_iter=5,           # イテレーション回数\n",
    "    random_state=0,\n",
    "    initial_strategy='mean'\n",
    ")\n",
    "\n",
    "# --- 3. 補完＋結合を一気に実行 ---\n",
    "df_imputed = pd.concat([\n",
    "    pd.DataFrame(\n",
    "        imp.fit_transform(df_values),\n",
    "        columns=df_values.columns,\n",
    "        index=df.index\n",
    "    ),\n",
    "    idx_event\n",
    "], axis=1)\n",
    "\n",
    "# --- 4. 結果確認 ---\n",
    "print(df_imputed.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a34162af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完全データセットをエクセルファイルに保存\n",
    "df_imputed.to_excel('completed_data_20250729.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51530b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "completed_data = pd.read_excel(r\"C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\completed_data_20250729.xlsx\") \n",
    "print(completed_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65f3d167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set\n",
      "Train      28561\n",
      "Holdout     7141\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. ランダムシード\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# 2. completed_data のコピーと明示的 INDEX 列（すでにある場合はスキップ）\n",
    "completed_data = completed_data.copy()\n",
    "if \"INDEX\" not in completed_data.columns:\n",
    "    completed_data[\"INDEX\"] = completed_data.index\n",
    "\n",
    "# 3. Stratified split を実施\n",
    "train_df, holdout_df = train_test_split(\n",
    "    completed_data,\n",
    "    test_size=0.2,\n",
    "    stratify=completed_data[\"Event\"],\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 4. INDEX による突合用データフレーム作成\n",
    "split_info = pd.DataFrame({\n",
    "    \"INDEX\": completed_data[\"INDEX\"],\n",
    "    \"Set\": np.where(completed_data[\"INDEX\"].isin(train_df[\"INDEX\"]), \"Train\", \"Holdout\")\n",
    "})\n",
    "\n",
    "# 5. completed_data に突合（マージ）\n",
    "completed_data = completed_data.merge(split_info, on=\"INDEX\", how=\"left\")\n",
    "\n",
    "# 6. 保存（任意）\n",
    "completed_data.to_csv(\"completed_data_with_split.csv\", index=False)\n",
    "\n",
    "# 7. 結果確認（オプション）\n",
    "print(completed_data[\"Set\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbe3250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Block-1 fully revised  v12  （2025-07-31 INDEX-key fix）\n",
    "───────────────────────────────────────────────────────\n",
    " * INDEX 列をインデックスにしてキー不整合を解消\n",
    " * bootstrap / F1 関連を numpy 配列で処理（KeyError 回避）\n",
    " * .values 参照を削除\n",
    "───────────────────────────────────────────────────────\n",
    "\"\"\"\n",
    "\n",
    "# ───────── Imports\n",
    "from __future__ import annotations\n",
    "import warnings, random, pickle, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as _st\n",
    "import optuna, xgboost as xgb, lightgbm as lgb\n",
    "\n",
    "from sklearn.base          import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose       import ColumnTransformer\n",
    "from sklearn.pipeline      import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, LeaveOneGroupOut\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute        import SimpleImputer\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.calibration   import CalibratedClassifierCV\n",
    "from sklearn.metrics       import roc_auc_score, average_precision_score, brier_score_loss, \\\n",
    "                                 precision_recall_curve, f1_score\n",
    "from category_encoders      import TargetEncoder\n",
    "\n",
    "# ───────── Globals\n",
    "RANDOM_STATE   = 42\n",
    "N_SPLITS_TUNE  = 5\n",
    "N_TRIALS       = 100\n",
    "PATIENCE       = 20\n",
    "N_BOOTSTRAP    = 3000\n",
    "\n",
    "np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "RESULTS_DIR = Path(f\"results_{datetime.now():%Y%m%d_%H%M}\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ───────── Transformer\n",
    "class WinsorizeLog1pTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, p_low=.005, p_high=.995, skew_thr=1.0):\n",
    "        self.p_low, self.p_high, self.skew_thr = p_low, p_high, skew_thr\n",
    "    def fit(self, X, y=None):\n",
    "        df = pd.DataFrame(X).astype(float)\n",
    "        self.q_ = {c: df[c].quantile([self.p_low, self.p_high]).values for c in df}\n",
    "        self.logmask_ = {c: (df[c]>=0).all() and abs(df[c].skew())>self.skew_thr for c in df}\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = pd.DataFrame(X).astype(float).copy()\n",
    "        for c in df:\n",
    "            lo, hi = self.q_[c]\n",
    "            df[c]  = df[c].clip(lo, hi)\n",
    "            if self.logmask_[c]: df[c] = np.log1p(df[c])\n",
    "        return df.values\n",
    "\n",
    "# ───────── GPU flags\n",
    "def xgb_gpu_params()->Dict:\n",
    "    return dict(tree_method=\"gpu_hist\", predictor=\"gpu_predictor\",\n",
    "                device=\"cuda\", deterministic=True, random_state=RANDOM_STATE)\n",
    "def lgb_gpu_flag()->Dict:\n",
    "    try: return dict(device=\"gpu\", deterministic=True)\n",
    "    except: return {}\n",
    "\n",
    "# ───────── Metric helpers\n",
    "def calib_slope_int(y, p):\n",
    "    eps = 1e-15\n",
    "    p   = np.clip(p, eps, 1-eps)\n",
    "    log = np.log(p/(1-p)).reshape(-1,1)\n",
    "    lr  = LogisticRegression(random_state=RANDOM_STATE).fit(log, y)\n",
    "    return lr.coef_[0][0], lr.intercept_[0]\n",
    "\n",
    "def _metric_set(y, p):\n",
    "    return (roc_auc_score(y, p),\n",
    "            average_precision_score(y, p),\n",
    "            brier_score_loss(y, p),\n",
    "            *calib_slope_int(y, p))\n",
    "\n",
    "def bootstrap_ci(y, p, n_boot=N_BOOTSTRAP, seed=RANDOM_STATE, alpha=.05):\n",
    "    y = np.asarray(y); p = np.asarray(p)\n",
    "    rng   = np.random.RandomState(seed)\n",
    "    idx   = np.arange(len(y))\n",
    "    stats = np.array([_metric_set(y[s], p[s])\n",
    "                      for s in rng.choice(idx, (n_boot, len(idx)), True)])\n",
    "    pt    = _metric_set(y, p)\n",
    "    lo, hi = np.percentile(stats, [100*alpha/2, 100*(1-alpha/2)], axis=0)\n",
    "    keys  = [\"AUROC\",\"PR_AUC\",\"Brier\",\"CalSlope\",\"CalInt\"]\n",
    "    return {k:(pt[i], lo[i], hi[i]) for i,k in enumerate(keys)}\n",
    "\n",
    "def best_f1_threshold(y, p):\n",
    "    y = np.asarray(y); p = np.asarray(p)\n",
    "    pr, rc, thr = precision_recall_curve(y, p)\n",
    "    f1          = 2*pr*rc/(pr+rc+1e-15)\n",
    "    i           = int(np.nanargmax(f1))\n",
    "    thr_best    = thr[i] if i < len(thr) else 1.0\n",
    "    return float(thr_best), float(f1[i])\n",
    "\n",
    "# ───────── DeLong (unchanged)\n",
    "def _compute_midrank(x):\n",
    "    J = np.argsort(x); Z = x[J]; N=len(x); T=np.zeros(N); i=0\n",
    "    while i < N:\n",
    "        j=i\n",
    "        while j<N and Z[j]==Z[i]: j+=1\n",
    "        T[i:j]=.5*(i+j-1); i=j\n",
    "    T2=np.empty(N); T2[J]=T+1; return T2\n",
    "def _fast_delong(preds_sorted_T, m):\n",
    "    n  = preds_sorted_T.shape[1]-m; k=preds_sorted_T.shape[0]\n",
    "    v01=np.zeros((k,m)); v10=np.zeros((k,n)); aucs=np.empty(k)\n",
    "    for r in range(k):\n",
    "        x,y=preds_sorted_T[r,:m],preds_sorted_T[r,m:]\n",
    "        tx=_compute_midrank(np.r_[x,y])[:m]; ty=_compute_midrank(np.r_[x,y])[m:]\n",
    "        aucs[r]=(tx.sum()-m*(m+1)/2)/(m*n)\n",
    "        v01[r]=(tx-tx.mean())/n; v10[r]=1-(ty-ty.mean())/m\n",
    "    S=np.cov(v01)/m + np.cov(v10)/n\n",
    "    return aucs,S\n",
    "def delong_roc_test(y,p1,p2):\n",
    "    y,p1,p2 = map(np.asarray,(y,p1,p2)); m=int((y==1).sum())\n",
    "    order   = np.argsort(-p1)\n",
    "    p1s,p2s,ys = p1[order],p2[order],y[order]\n",
    "    pos,neg    = np.where(ys==1)[0], np.where(ys==0)[0]\n",
    "    aucs,S     = _fast_delong(np.vstack([np.r_[p1s[pos],p1s[neg]],\n",
    "                                         np.r_[p2s[pos],p2s[neg]]]), m)\n",
    "    diff = aucs[1]-aucs[0]; var=S[0,0]+S[1,1]-2*S[0,1]\n",
    "    z    = np.abs(diff)/math.sqrt(var) if var>0 else np.inf\n",
    "    return diff, 2*(1-_st.norm.cdf(z))\n",
    "\n",
    "# ───────── Estimator & pipeline builders\n",
    "def build_estimator(name, pos_w):\n",
    "    if name == \"Logistic\":\n",
    "        mdl = LogisticRegression(solver=\"liblinear\", max_iter=1000,\n",
    "                                 random_state=RANDOM_STATE)\n",
    "        space = {\n",
    "            \"model__C\":\n",
    "              optuna.distributions.FloatDistribution(1e-3, 1e1, log=True),\n",
    "            \"model__class_weight\":\n",
    "              optuna.distributions.CategoricalDistribution([None,\"balanced\"])\n",
    "        }\n",
    "    elif name == \"XGBoost\":\n",
    "        low, hi = .5*pos_w, 1.5*pos_w\n",
    "        mdl = xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric=\"aucpr\",\n",
    "                                n_estimators=1200, **xgb_gpu_params())\n",
    "        space = {\n",
    "            \"model__max_depth\":\n",
    "              optuna.distributions.IntDistribution(3,9),\n",
    "            \"model__learning_rate\":\n",
    "              optuna.distributions.FloatDistribution(1e-3,3e-2,log=True),\n",
    "            \"model__min_child_weight\":\n",
    "              optuna.distributions.IntDistribution(1,10),\n",
    "            \"model__gamma\":\n",
    "              optuna.distributions.FloatDistribution(0,5),\n",
    "            \"model__subsample\":\n",
    "              optuna.distributions.FloatDistribution(.5,.9),\n",
    "            \"model__colsample_bytree\":\n",
    "              optuna.distributions.FloatDistribution(.5,.9),\n",
    "            \"model__reg_alpha\":\n",
    "              optuna.distributions.FloatDistribution(1e-3,10,log=True),\n",
    "            \"model__reg_lambda\":\n",
    "              optuna.distributions.FloatDistribution(1e-3,10,log=True),\n",
    "            \"model__scale_pos_weight\":\n",
    "              optuna.distributions.FloatDistribution(low,hi),\n",
    "            \"model__n_estimators\":\n",
    "              optuna.distributions.IntDistribution(300,800,step=100)\n",
    "        }\n",
    "    else:  # LightGBM\n",
    "        low, hi = .5*pos_w, 1.5*pos_w\n",
    "        mdl = lgb.LGBMClassifier(objective=\"binary\", metric=\"aucpr\",\n",
    "                                 n_estimators=1200, random_state=RANDOM_STATE,\n",
    "                                 **lgb_gpu_flag())\n",
    "        space = {\n",
    "            \"model__num_leaves\":\n",
    "              optuna.distributions.IntDistribution(31,255),\n",
    "            \"model__learning_rate\":\n",
    "              optuna.distributions.FloatDistribution(1e-3,3e-2,log=True),\n",
    "            \"model__feature_fraction\":\n",
    "              optuna.distributions.FloatDistribution(.5,.9),\n",
    "            \"model__bagging_fraction\":\n",
    "              optuna.distributions.FloatDistribution(.5,.9),\n",
    "            \"model__bagging_freq\":\n",
    "              optuna.distributions.IntDistribution(1,7),\n",
    "            \"model__lambda_l1\":\n",
    "              optuna.distributions.FloatDistribution(1e-3,10,log=True),\n",
    "            \"model__lambda_l2\":\n",
    "              optuna.distributions.FloatDistribution(1e-3,10,log=True),\n",
    "            \"model__scale_pos_weight\":\n",
    "              optuna.distributions.FloatDistribution(low,hi),\n",
    "            \"model__n_estimators\":\n",
    "              optuna.distributions.IntDistribution(300,800,step=100)\n",
    "        }\n",
    "    return mdl, space\n",
    "\n",
    "def build_pipe(model, cat_cols, num_cols):\n",
    "    transformers=[]\n",
    "    if cat_cols:\n",
    "        cat_pipe = Pipeline([\n",
    "            (\"enc\",TargetEncoder(cols=cat_cols,smoothing=0.3,\n",
    "                                 handle_unknown=\"impute\")),\n",
    "            (\"imp\",SimpleImputer(strategy=\"most_frequent\"))\n",
    "        ])\n",
    "        transformers.append((\"cat\",cat_pipe,cat_cols))\n",
    "    if num_cols:\n",
    "        num_pipe = Pipeline([\n",
    "            (\"win\",WinsorizeLog1pTransformer()),\n",
    "            (\"imp\",SimpleImputer(strategy=\"median\")),\n",
    "            (\"sc\",RobustScaler())\n",
    "        ])\n",
    "        transformers.append((\"num\",num_pipe,num_cols))\n",
    "    pre = ColumnTransformer(transformers,\n",
    "                            remainder=\"passthrough\" if not transformers else \"drop\",\n",
    "                            n_jobs=1)\n",
    "    return Pipeline([(\"pre\",pre),(\"model\",model)])\n",
    "\n",
    "# ───────── CV + early-stopping\n",
    "def cv_predict_proba_es(pipe, X:pd.DataFrame, y, n_splits=N_SPLITS_TUNE):\n",
    "    cv   = StratifiedKFold(n_splits=n_splits, shuffle=True,\n",
    "                           random_state=RANDOM_STATE)\n",
    "    oof  = np.empty(len(y))\n",
    "    for tr,va in cv.split(X, y):\n",
    "        X_tr,y_tr = X.iloc[tr], y[tr]\n",
    "        X_va,y_va = X.iloc[va], y[va]\n",
    "        mdl_cls   = type(pipe.named_steps[\"model\"])\n",
    "        if mdl_cls in (xgb.XGBModel, lgb.LGBMModel):\n",
    "            pipe.fit(X_tr,y_tr,\n",
    "                     model__eval_set=[(X_va,y_va)],\n",
    "                     model__eval_metric=\"aucpr\",\n",
    "                     model__early_stopping_rounds=50,\n",
    "                     model__verbose=False)\n",
    "        else:\n",
    "            pipe.fit(X_tr,y_tr)\n",
    "        oof[va] = pipe.predict_proba(X_va)[:,1]\n",
    "    return oof\n",
    "\n",
    "# ───────── Optuna tuner\n",
    "def tune(pipe, space, X:pd.DataFrame, y):\n",
    "    def objective(trial):\n",
    "        for prm, dist in space.items():\n",
    "            if isinstance(dist, optuna.distributions.FloatDistribution):\n",
    "                pipe.set_params(**{\n",
    "                    prm: trial.suggest_float(prm, dist.low, dist.high, log=dist.log)})\n",
    "            elif isinstance(dist, optuna.distributions.IntDistribution):\n",
    "                pipe.set_params(**{\n",
    "                    prm: trial.suggest_int(prm, dist.low, dist.high, step=dist.step)})\n",
    "            else:\n",
    "                pipe.set_params(**{\n",
    "                    prm: trial.suggest_categorical(prm, dist.choices)})\n",
    "        p = cv_predict_proba_es(pipe, X, y)\n",
    "        return average_precision_score(y, p)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\",\n",
    "                                sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n",
    "                                pruner=optuna.pruners.MedianPruner(n_warmup_steps=10))\n",
    "    study.optimize(objective, n_trials=N_TRIALS,\n",
    "                   callbacks=[lambda s,t:\n",
    "                              s.stop() if t.number-s.best_trial.number>=PATIENCE else None],\n",
    "                   show_progress_bar=False)\n",
    "    pipe.set_params(**study.best_params)\n",
    "    best = {k.replace(\"model__\",\"\"):v for k,v in study.best_params.items()}\n",
    "    return pipe, best\n",
    "\n",
    "# ───────── Hold-out evaluation\n",
    "def experiment_metrics(model_name, Xd:pd.DataFrame, yd, Xh:pd.DataFrame, yh,\n",
    "                       cat, num):\n",
    "    pos_w=(yd==0).sum()/max((yd==1).sum(),1)\n",
    "    base,space = build_estimator(model_name, pos_w)\n",
    "    pipe,best  = tune(build_pipe(base, cat, num), space, Xd, yd)\n",
    "\n",
    "    final = CalibratedClassifierCV(pipe, method=\"isotonic\", cv=N_SPLITS_TUNE)\n",
    "    final.fit(Xd, yd)\n",
    "    p = final.predict_proba(Xh)[:,1]\n",
    "\n",
    "    ci  = bootstrap_ci(yh, p)\n",
    "    thr, f1_pt = best_f1_threshold(yh, p)\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    f1_bs = [f1_score(yh[s], (p[s]>=thr).astype(int))\n",
    "             for s in rng.choice(np.arange(len(yh)),\n",
    "                                 (N_BOOTSTRAP, len(yh)), True)]\n",
    "    lo_f1, hi_f1 = np.percentile(f1_bs, [2.5,97.5])\n",
    "    ci[\"F1_best\"] = (f1_pt, lo_f1, hi_f1)\n",
    "    ci[\"F1_thr\"]  = (thr, np.nan, np.nan)\n",
    "\n",
    "    flat = {k:pt for k,(pt,_,_) in ci.items()}\n",
    "    for k,(pt,lo,hi) in ci.items():\n",
    "        flat[f\"{k}_lo\"] = lo; flat[f\"{k}_hi\"] = hi\n",
    "    return p, flat, best, final\n",
    "\n",
    "# ───────── LOGO / facility\n",
    "def calculate_logo_metrics(model, X:pd.DataFrame, y, cat, num, groups, best):\n",
    "    pos_w=(y==0).sum()/max((y==1).sum(),1)\n",
    "    base,_ = build_estimator(model, pos_w)\n",
    "    pipe   = build_pipe(base, cat, num)\n",
    "    pipe.set_params(**{f\"model__{k}\":v for k,v in best.items()})\n",
    "    calib  = CalibratedClassifierCV(pipe, method=\"isotonic\", cv=N_SPLITS_TUNE)\n",
    "\n",
    "    preds = np.empty(len(y))\n",
    "    logo  = LeaveOneGroupOut()\n",
    "    for tr,te in logo.split(X, y, groups):\n",
    "        calib.fit(X.iloc[tr], y[tr])\n",
    "        preds[te] = calib.predict_proba(X.iloc[te])[:,1]\n",
    "\n",
    "    ci  = bootstrap_ci(y, preds)\n",
    "    thr, f1_pt = best_f1_threshold(y, preds)\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    f1_bs = [f1_score(y[s], (preds[s]>=thr).astype(int))\n",
    "             for s in rng.choice(np.arange(len(y)), (N_BOOTSTRAP,len(y)), True)]\n",
    "    lo_f1, hi_f1 = np.percentile(f1_bs, [2.5,97.5])\n",
    "    ci[\"F1_best\"] = (f1_pt, lo_f1, hi_f1)\n",
    "    ci[\"F1_thr\"]  = (thr, np.nan, np.nan)\n",
    "\n",
    "    out={}\n",
    "    for k,(pt,lo,hi) in ci.items():\n",
    "        out[f\"LOGO_{k}\"]=pt\n",
    "        out[f\"LOGO_{k}_lo\"]=lo\n",
    "        out[f\"LOGO_{k}_hi\"]=hi\n",
    "    return out, preds\n",
    "\n",
    "def calculate_facility_metrics(model, X_df:pd.DataFrame, y, cat, num,\n",
    "                               groups, best):\n",
    "    res={}\n",
    "    for fid in np.sort(groups.unique()):\n",
    "        tr, te = (groups!=fid), (groups==fid)\n",
    "        pos_w  = (y[tr]==0).sum()/max((y[tr]==1).sum(),1)\n",
    "        base,_ = build_estimator(model,pos_w)\n",
    "        pipe   = build_pipe(base, cat, num)\n",
    "        pipe.set_params(**{f\"model__{k}\":v for k,v in best.items()})\n",
    "        calib  = CalibratedClassifierCV(pipe, method=\"isotonic\", cv=N_SPLITS_TUNE)\n",
    "        calib.fit(X_df.loc[tr], y[tr])\n",
    "        p = calib.predict_proba(X_df.loc[te])[:,1]\n",
    "\n",
    "        ci  = bootstrap_ci(y[te], p)\n",
    "        thr, f1_pt = best_f1_threshold(y[te], p)\n",
    "        rng = np.random.RandomState(RANDOM_STATE)\n",
    "        f1_bs = [f1_score(y[te][s], (p[s]>=thr).astype(int))\n",
    "                 for s in rng.choice(np.arange(len(y[te])),\n",
    "                                     (N_BOOTSTRAP,len(y[te])), True)]\n",
    "        lo_f1, hi_f1 = np.percentile(f1_bs, [2.5,97.5])\n",
    "        ci[\"F1_best\"] = (f1_pt, lo_f1, hi_f1)\n",
    "        ci[\"F1_thr\"]  = (thr, np.nan, np.nan)\n",
    "\n",
    "        for k,(pt,lo,hi) in ci.items():\n",
    "            res[f\"Fac{fid}_{k}\"]    = pt\n",
    "            res[f\"Fac{fid}_{k}_lo\"] = lo\n",
    "            res[f\"Fac{fid}_{k}_hi\"] = hi\n",
    "    return res\n",
    "\n",
    "# ───────── Main\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_PATH = r\"C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\completed_data_20250729.xlsx\"\n",
    "    df = pd.read_excel(DATA_PATH).set_index(\"INDEX\")     # ← INDEX をキーに\n",
    "    df[\"付属_1\"] = df[\"付属_1\"].astype(int)\n",
    "\n",
    "    CAT_PRE = [ \"Male\",\"Dialysis\",\"CHF\",\"Malig\",\"β-blocker\",\"Oral steroids\",\n",
    "                \"DeliMed\",\"Anticoag\",\"Antiplatelet\",\"AntiCa\",\"Opioid\",\n",
    "                \"Proc-Eye\",\"Proc-Face/Neck\",\"Proc-Thorax\",\"Proc-MSK\",\"Proc-ENT\",\n",
    "                \"Proc-Neuro\",\"Proc-Genital\",\"Proc-Urinary\",\"Proc-Skin\",\"Proc-Abd\",\n",
    "                \"ResectNum\",\"HighRiskProc\" ]\n",
    "    NUM_PRE = [ \"ASA\",\"Age\",\"BMI\",\"Alb\",\"BUN\",\"CRP\",\"Cre\",\"Hb\",\"K\",\"Na\",\"PLT\",\n",
    "                \"T-Bil\",\"WBC\" ]\n",
    "    CAT_INTRA = CAT_PRE.copy()\n",
    "    NUM_INTRA = NUM_PRE + [\"RBC Tx\",\"FFP Tx\",\"PLT Tx\",\"FluidBal\",\n",
    "                           \"OpTime\",\"HR at 6h\",\"MAP at 6h\"]\n",
    "\n",
    "    VARIANTS = {\n",
    "        \"①Pre-op\":          (CAT_PRE, NUM_PRE),\n",
    "        \"②Peri-op+Intra\":   (CAT_INTRA, NUM_INTRA)\n",
    "    }\n",
    "    MODELS = [\"Logistic\",\"XGBoost\",\"LightGBM\"]\n",
    "\n",
    "    summary, hparams = [], []\n",
    "\n",
    "    for variant,(cat_raw,num_raw) in VARIANTS.items():\n",
    "        print(f\"\\n=== {variant} ===\")\n",
    "        CAT = [c for c in cat_raw if c in df.columns]\n",
    "        NUM = [n for n in num_raw if n in df.columns]\n",
    "        if not CAT and not NUM:\n",
    "            raise ValueError(f\"{variant}: feature 列が見つかりません\")\n",
    "\n",
    "        dev, hold = train_test_split(df,\n",
    "                                     test_size=0.20,\n",
    "                                     stratify=df[\"Event\"],\n",
    "                                     random_state=RANDOM_STATE)\n",
    "\n",
    "        Xd, yd = dev.drop(\"Event\",axis=1), dev[\"Event\"].values\n",
    "        Xh, yh = hold.drop(\"Event\",axis=1), hold[\"Event\"].values\n",
    "\n",
    "        preds_hold = {}\n",
    "        for model in MODELS:\n",
    "            print(\"   ▸\", model)\n",
    "            p_hold, hold_met, best, clf = experiment_metrics(\n",
    "                model, Xd, yd, Xh, yh, CAT, NUM)\n",
    "            preds_hold[model] = p_hold\n",
    "\n",
    "            logo_met, _ = calculate_logo_metrics(\n",
    "                model, df.drop(\"Event\",axis=1), df[\"Event\"].values,\n",
    "                CAT, NUM, df[\"付属_1\"], best)\n",
    "            fac_met = calculate_facility_metrics(\n",
    "                model, df.drop(\"Event\",axis=1), df[\"Event\"].values,\n",
    "                CAT, NUM, df[\"付属_1\"], best)\n",
    "\n",
    "            au_diff, delong_p = 0.0, 0.0\n",
    "            if model != \"Logistic\":\n",
    "                au_diff, delong_p = delong_roc_test(\n",
    "                    yh, preds_hold[\"Logistic\"], p_hold)\n",
    "\n",
    "            rec = {\"Variant\": variant, \"Model\": model,\n",
    "                   **{f\"Hold_{k}\": hold_met[k] for k in hold_met},\n",
    "                   **logo_met, **fac_met,\n",
    "                   \"Hold_p_vs_Logit\": delong_p,\n",
    "                   \"Hold_AUROC_diff\": au_diff}\n",
    "            summary.append(rec)\n",
    "            hparams.append({\"Variant\": variant, \"Model\": model, **best})\n",
    "\n",
    "            with open(RESULTS_DIR/f\"{variant}_{model}_hold_model.pkl\",\"wb\") as f:\n",
    "                pickle.dump(clf, f)\n",
    "            pd.DataFrame({\"Index\": hold.index,\n",
    "                          \"y_true\": yh, \"y_pred\": p_hold})\\\n",
    "                .to_csv(RESULTS_DIR/f\"{variant}_{model}_hold_preds.csv\",\n",
    "                        index=False)\n",
    "\n",
    "        # split map\n",
    "        pd.concat([\n",
    "            pd.DataFrame({\"Index\": dev.index,  \"Split\":\"train\"}),\n",
    "            pd.DataFrame({\"Index\": hold.index, \"Split\":\"test\"})\n",
    "        ]).sort_values(\"Index\")\\\n",
    "         .to_excel(RESULTS_DIR/f\"{variant}_split_map.xlsx\", index=False)\n",
    "\n",
    "    pd.DataFrame(summary)\\\n",
    "      .to_excel(RESULTS_DIR/\"summary_metrics.xlsx\", index=False,\n",
    "                float_format=\"%.4f\")\n",
    "    df_hp = pd.DataFrame(hparams)\n",
    "    cols  = [\"Variant\",\"Model\"] + sorted(\n",
    "              c for c in df_hp.columns if c not in [\"Variant\",\"Model\"])\n",
    "    df_hp[cols].to_excel(RESULTS_DIR/\"selected_hyperparameters.xlsx\",\n",
    "                         index=False)\n",
    "\n",
    "    print(\"\\n✓ Finished →\", RESULTS_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f839b5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Non-SHAP plots saved → C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\results_20250802_0850\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "non_shap_plots.py  – 2025-08-01 order-fixed (LR→XGB→LGBM)\n",
    "                    FINAL: Configured variant display names for final plots.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, warnings, pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.metrics       import precision_recall_curve, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ───────── Const\n",
    "RESULTS_DIR  = Path(r\"C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\results_20250802_0850\")\n",
    "DATA_PATH    = Path(r\"C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\completed_data_20250729.xlsx\")\n",
    "RANDOM_STATE = 42\n",
    "MODEL_ORDER  = {\"Logistic\": 0, \"XGBoost\": 1, \"LightGBM\": 2}\n",
    "\n",
    "# Model name abbreviations\n",
    "MODEL_ABBREVIATIONS = {\n",
    "    \"Logistic\": \"LR\",\n",
    "    \"XGBoost\": \"XGB\",\n",
    "    \"LightGBM\": \"LGBM\"\n",
    "}\n",
    "\n",
    "# ★★★ 最終的な設定 ★★★\n",
    "# 検出されたvariant名と、表示したいグラフ上の名前を対応させます。\n",
    "VARIANT_DISPLAY_NAMES = {\n",
    "    \"①Pre-op\": \"①Pre-op\",\n",
    "    \"②Peri-op+Intra\": \"②Peri-op\"\n",
    "}\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ───────── Helper\n",
    "def load_pipeline(pkl_path: Path):\n",
    "    mdl = pickle.load(open(pkl_path, \"rb\"))\n",
    "    if isinstance(mdl, CalibratedClassifierCV):\n",
    "        return mdl.calibrated_classifiers_[0].estimator\n",
    "    return mdl\n",
    "\n",
    "def net_benefit(y: np.ndarray, p: np.ndarray, thr: float) -> float:\n",
    "    n  = len(y)\n",
    "    tp = ((p >= thr) & (y == 1)).sum()\n",
    "    fp = ((p >= thr) & (y == 0)).sum()\n",
    "    return tp / n - fp / n * (thr / (1 - thr))\n",
    "\n",
    "# ───────── 1. モデル & 予測読み込み\n",
    "models_info: List[Dict] = []\n",
    "for pkl in RESULTS_DIR.glob(\"*_hold_model.pkl\"):\n",
    "    stem = pkl.stem\n",
    "    if not stem.endswith(\"_hold_model\"):\n",
    "        continue\n",
    "    base = stem[:-11]\n",
    "    if \"_\" not in base:\n",
    "        continue\n",
    "    variant, model = base.rsplit(\"_\", 1)\n",
    "    csv = pkl.with_name(base + \"_hold_preds.csv\")\n",
    "    if not csv.exists():\n",
    "        continue\n",
    "    dfp = pd.read_csv(csv)\n",
    "    if {\"y_true\", \"y_pred\"}.issubset(dfp.columns):\n",
    "        models_info.append(dict(\n",
    "            variant=variant, model=model,\n",
    "            pkl=pkl,\n",
    "            y_true=dfp.y_true.values,\n",
    "            y_pred=dfp.y_pred.values\n",
    "        ))\n",
    "\n",
    "if not models_info:\n",
    "    raise RuntimeError(\"対象ファイルが見つかりません。\")\n",
    "\n",
    "# 指定順(Logistic→XGB→LGBM)で並べ替え\n",
    "models_info.sort(\n",
    "    key=lambda d: (VARIANT_DISPLAY_NAMES.get(d[\"variant\"], d[\"variant\"]), MODEL_ORDER.get(d[\"model\"], 99))\n",
    ")\n",
    "\n",
    "# ───────── 2. hold-out prevalence\n",
    "df_all = pd.read_excel(DATA_PATH)\n",
    "_, hold = train_test_split(df_all, test_size=0.2,\n",
    "                           stratify=df_all.Event, random_state=RANDOM_STATE)\n",
    "prev = hold.Event.mean()\n",
    "\n",
    "# ───────── 3. Precision–Recall\n",
    "plt.figure(figsize=(8, 8))\n",
    "for info in models_info:\n",
    "    prec, rec, _ = precision_recall_curve(info[\"y_true\"], info[\"y_pred\"])\n",
    "    display_variant = VARIANT_DISPLAY_NAMES.get(info[\"variant\"], info[\"variant\"])\n",
    "    model_abbr = MODEL_ABBREVIATIONS.get(info[\"model\"], info[\"model\"])\n",
    "    plt.plot(rec, prec, label=f'{display_variant}-{model_abbr}')\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve (hold-out)\"); plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"PR_Curves_AllModels.jpeg\", dpi=600)\n",
    "plt.close()\n",
    "\n",
    "# ───────── 4. Calibration\n",
    "plt.figure(figsize=(8, 8))\n",
    "for info in models_info:\n",
    "    obs, pred = calibration_curve(info[\"y_true\"], info[\"y_pred\"], n_bins=10)\n",
    "    display_variant = VARIANT_DISPLAY_NAMES.get(info[\"variant\"], info[\"variant\"])\n",
    "    model_abbr = MODEL_ABBREVIATIONS.get(info[\"model\"], info[\"model\"])\n",
    "    plt.plot(pred, obs, \"o-\", label=f'{display_variant}-{model_abbr}')\n",
    "plt.plot([0, 1], [0, 1], \"--\", color=\"gray\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"Observed\")\n",
    "plt.title(\"Calibration (hold-out)\"); plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"Calibration_Curves_AllModels.jpeg\", dpi=600)\n",
    "plt.close()\n",
    "\n",
    "# ───────── 5. ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "for info in models_info:\n",
    "    fpr, tpr, _ = roc_curve(info[\"y_true\"], info[\"y_pred\"])\n",
    "    display_variant = VARIANT_DISPLAY_NAMES.get(info[\"variant\"], info[\"variant\"])\n",
    "    model_abbr = MODEL_ABBREVIATIONS.get(info[\"model\"], info[\"model\"])\n",
    "    plt.plot(fpr, tpr, label=f'{display_variant}-{model_abbr}')\n",
    "plt.plot([0, 1], [0, 1], \"--\", color=\"gray\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
    "plt.title(\"AUROC (hold-out)\"); plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"AUROC_Curves_AllModels.jpeg\", dpi=600)\n",
    "plt.close()\n",
    "\n",
    "# ───────── 6. DCA\n",
    "threshs = np.linspace(0.01, 0.5, 50)\n",
    "plt.figure(figsize=(8, 6))\n",
    "for info in models_info:\n",
    "    nb = [net_benefit(info[\"y_true\"], info[\"y_pred\"], t) for t in threshs]\n",
    "    display_variant = VARIANT_DISPLAY_NAMES.get(info[\"variant\"], info[\"variant\"])\n",
    "    model_abbr = MODEL_ABBREVIATIONS.get(info[\"model\"], info[\"model\"])\n",
    "    plt.plot(threshs, nb, label=f'{display_variant}-{model_abbr}')\n",
    "plt.plot(threshs, np.zeros_like(threshs), \"--\", label=\"Treat None\")\n",
    "plt.plot(threshs, prev - (1 - prev) * (threshs / (1 - threshs)),\n",
    "         \"--\", label=\"Treat All\")\n",
    "plt.xlabel(\"Threshold\"); plt.ylabel(\"Net benefit\")\n",
    "plt.title(\"Decision Curve (hold-out)\")\n",
    "plt.xlim(0, 0.5); plt.ylim(-0.15, 0.15)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"DCA_AllModels.jpeg\", dpi=600)\n",
    "plt.close()\n",
    "\n",
    "print(f\"✓ Non-SHAP plots saved → {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "sensitivity_block_v14_fixed.py   (2025-08-01  no-early-stop safe, TS uses COMPL_PATH)\n",
    "───────────────────────────────────────────────────────────\n",
    " * CC / PI-only / TS  の 3 モード一括感度解析\n",
    " * Pre-op / Peri-op+Intra 2 variant × Logistic / XGB / LGB\n",
    " * early-stopping / eval_set / callbacks → **完全に削除**\n",
    " * hold-out のみ出力（施設別・LOGO 検証なし）\n",
    " * TS モードでも COMPL_PATH を使用\n",
    "\"\"\"\n",
    "\n",
    "# ───── Imports\n",
    "from __future__ import annotations\n",
    "import warnings, random, pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np, pandas as pd, optuna, xgboost as xgb, lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.compose         import ColumnTransformer\n",
    "from sklearn.preprocessing   import RobustScaler\n",
    "from sklearn.impute          import SimpleImputer\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.calibration     import CalibratedClassifierCV\n",
    "from sklearn.metrics         import (roc_auc_score, average_precision_score,\n",
    "                                     brier_score_loss, precision_recall_curve,\n",
    "                                     f1_score)\n",
    "from category_encoders        import TargetEncoder\n",
    "\n",
    "# ───── Globals\n",
    "RANDOM_STATE   = 42\n",
    "N_TRIALS       = 100\n",
    "PATIENCE       = 20\n",
    "N_SPLITS_TUNE  = 5\n",
    "N_BOOTSTRAP    = 1500\n",
    "PI_COL         = \"PI\"\n",
    "\n",
    "RAW_PATH   = r\"C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\ver1_20250729.xlsx\"\n",
    "COMPL_PATH = r\"C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\completed_data_20250729.xlsx\"\n",
    "\n",
    "OUTDIR = Path(f\"sens_results_{datetime.now():%Y%m%d_%H%M}\")\n",
    "OUTDIR.mkdir(exist_ok=True)\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ───── 1. 列定義\n",
    "CAT_PRE = [\"Male\",\"Dialysis\",\"CHF\",\"Malig\",\"β-blocker\",\"Oral steroids\",\"DeliMed\",\n",
    "           \"Anticoag\",\"Antiplatelet\",\"AntiCa\",\"Opioid\",\"Proc-Eye\",\"Proc-Face/Neck\",\n",
    "           \"Proc-Thorax\",\"Proc-MSK\",\"Proc-ENT\",\"Proc-Neuro\",\"Proc-Genital\",\n",
    "           \"Proc-Urinary\",\"Proc-Skin\",\"Proc-Abd\",\"ResectNum\",\"HighRiskProc\"]\n",
    "NUM_PRE = [\"ASA\",\"Age\",\"BMI\",\"Alb\",\"BUN\",\"CRP\",\"Cre\",\"Hb\",\"K\",\"Na\",\"PLT\",\"T-Bil\",\"WBC\"]\n",
    "\n",
    "CAT_INTRA = CAT_PRE.copy()\n",
    "NUM_INTRA = NUM_PRE + [\"RBC Tx\",\"FFP Tx\",\"PLT Tx\",\"FluidBal\",\n",
    "                       \"OpTime\",\"HR at 6h\",\"MAP at 6h\"]\n",
    "\n",
    "VARIANTS = {\"①Pre-op\": (CAT_PRE, NUM_PRE),\n",
    "            \"②Peri-op+Intra\": (CAT_INTRA, NUM_INTRA)}\n",
    "MODELS = [\"Logistic\",\"XGBoost\",\"LightGBM\"]\n",
    "\n",
    "# ───── 2. Data utilities\n",
    "def drop_missing_cc(df): \n",
    "    return df.dropna(how=\"any\").reset_index(drop=True)\n",
    "\n",
    "def restrict_to_pi(df, event_col=\"Event\"):\n",
    "    df = df.copy()\n",
    "    df[event_col] = df[PI_COL]\n",
    "    return df\n",
    "\n",
    "def identity(df): \n",
    "    return df\n",
    "\n",
    "MODES = {\"CC\": drop_missing_cc, \"PI\": restrict_to_pi, \"TS\": identity}\n",
    "\n",
    "# ───── 3. Transformer\n",
    "class WinsorLog1p(SimpleImputer):\n",
    "    def __init__(self, lo=.005, hi=.995, skew_thr=1.0):\n",
    "        super().__init__(strategy=\"median\")\n",
    "        self.lo, self.hi, self.skew_thr = lo, hi, skew_thr\n",
    "        self.scaler = RobustScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        df = pd.DataFrame(X).astype(float)\n",
    "        self.q_  = {c: df[c].quantile([self.lo, self.hi]).values for c in df}\n",
    "        self.log_ = {c: (df[c]>=0).all() and abs(df[c].skew())>self.skew_thr for c in df}\n",
    "        df = self._pre(df)\n",
    "        super().fit(df.values)\n",
    "        self.scaler.fit(super().transform(df.values))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = self._pre(pd.DataFrame(X).astype(float))\n",
    "        return self.scaler.transform(super().transform(df.values))\n",
    "\n",
    "    def _pre(self, df):\n",
    "        for c in df:\n",
    "            lo, hi = self.q_[c]\n",
    "            df[c] = df[c].clip(lo, hi)\n",
    "            if self.log_[c]:\n",
    "                df[c] = np.log1p(df[c])\n",
    "        return df\n",
    "\n",
    "# ───── 4. Metric helpers\n",
    "def _metrics(y, p):\n",
    "    eps = 1e-15\n",
    "    p_clip = np.clip(p, eps, 1-eps)\n",
    "    lr1 = LogisticRegression().fit(np.log(p_clip/(1-p_clip)).reshape(-1,1), y)\n",
    "    lr2 = LogisticRegression().fit(np.log(p_clip/(1-p_clip)).reshape(-1,1), y)\n",
    "    slope, intercept = lr1.coef_[0][0], lr2.intercept_[0]\n",
    "    return (roc_auc_score(y, p),\n",
    "            average_precision_score(y, p),\n",
    "            brier_score_loss(y, p),\n",
    "            slope, intercept)\n",
    "\n",
    "def ci_boot(y, p, n=N_BOOTSTRAP):\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    idx = np.arange(len(y))\n",
    "    stats = np.array([_metrics(y[s], p[s]) for s in rng.choice(idx, (n, len(idx)), True)])\n",
    "    pt = _metrics(y, p)\n",
    "    lo, hi = np.percentile(stats, [2.5, 97.5], axis=0)\n",
    "    keys = [\"AUROC\",\"PR_AUC\",\"Brier\",\"CalSlope\",\"CalInt\"]\n",
    "    return {k: (pt[i], lo[i], hi[i]) for i, k in enumerate(keys)}\n",
    "\n",
    "def best_f1(y, p):\n",
    "    pr, rc, thr = precision_recall_curve(y, p)\n",
    "    f1 = 2 * pr * rc / (pr + rc + 1e-15)\n",
    "    i = int(np.nanargmax(f1))\n",
    "    return (thr[i] if i < len(thr) else 1.0, f1[i])\n",
    "\n",
    "# ───── 5. Estimators & pipeline\n",
    "def xgb_gpu():\n",
    "    return dict(tree_method=\"gpu_hist\", predictor=\"gpu_predictor\",\n",
    "                device=\"cuda\", random_state=RANDOM_STATE,\n",
    "                enable_categorical=False)\n",
    "\n",
    "def lgb_gpu():\n",
    "    try:\n",
    "        return dict(device=\"gpu\")\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "def build_estimator(name, pos_w):\n",
    "    if name == \"Logistic\":\n",
    "        mdl = LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=RANDOM_STATE)\n",
    "        space = {\"model__C\": optuna.distributions.FloatDistribution(1e-3, 10, log=True)}\n",
    "    elif name == \"XGBoost\":\n",
    "        mdl = xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric=\"aucpr\",\n",
    "                                n_estimators=600, **xgb_gpu())\n",
    "        space = {\n",
    "            \"model__max_depth\": optuna.distributions.IntDistribution(3, 8),\n",
    "            \"model__learning_rate\": optuna.distributions.FloatDistribution(1e-3, 3e-2, log=True),\n",
    "            \"model__subsample\": optuna.distributions.FloatDistribution(.5, .9),\n",
    "            \"model__colsample_bytree\": optuna.distributions.FloatDistribution(.5, .9),\n",
    "            \"model__scale_pos_weight\": optuna.distributions.FloatDistribution(.5 * pos_w, 1.5 * pos_w)\n",
    "        }\n",
    "    else:  # LightGBM\n",
    "        mdl = lgb.LGBMClassifier(objective=\"binary\", metric=\"aucpr\",\n",
    "                                 n_estimators=600, random_state=RANDOM_STATE, **lgb_gpu())\n",
    "        space = {\n",
    "            \"model__num_leaves\": optuna.distributions.IntDistribution(31, 127),\n",
    "            \"model__learning_rate\": optuna.distributions.FloatDistribution(1e-3, 3e-2, log=True),\n",
    "            \"model__feature_fraction\": optuna.distributions.FloatDistribution(.5, .9),\n",
    "            \"model__bagging_fraction\": optuna.distributions.FloatDistribution(.5, .9),\n",
    "            \"model__bagging_freq\": optuna.distributions.IntDistribution(1, 5),\n",
    "            \"model__scale_pos_weight\": optuna.distributions.FloatDistribution(.5 * pos_w, 1.5 * pos_w)\n",
    "        }\n",
    "    return mdl, space\n",
    "\n",
    "def build_pipe(model, cat, num):\n",
    "    tr = []\n",
    "    if cat:\n",
    "        tr.append((\"cat\", Pipeline([\n",
    "            (\"enc\", TargetEncoder(cols=cat, smoothing=0.3, handle_unknown=\"impute\")),\n",
    "            (\"imp\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "        ]), cat))\n",
    "    if num:\n",
    "        tr.append((\"num\", Pipeline([\n",
    "            (\"win\", WinsorLog1p())\n",
    "        ]), num))\n",
    "    pre = ColumnTransformer(tr, remainder=\"drop\" if tr else \"passthrough\")\n",
    "    return Pipeline([(\"pre\", pre), (\"model\", model)])\n",
    "\n",
    "# ───── 6. CV (no early-stop, no eval_set)\n",
    "def cv_predict(pipe, X: pd.DataFrame, y):\n",
    "    cv = StratifiedKFold(n_splits=N_SPLITS_TUNE, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof = np.empty(len(y))\n",
    "    for tr, va in cv.split(X, y):\n",
    "        pipe.fit(X.iloc[tr], y[tr])\n",
    "        oof[va] = pipe.predict_proba(X.iloc[va])[:, 1]\n",
    "    return oof\n",
    "\n",
    "# ───── 7. Optuna tuner\n",
    "def tune(pipe, space, X, y):\n",
    "    def obj(trial):\n",
    "        for prm, dist in space.items():\n",
    "            if isinstance(dist, optuna.distributions.FloatDistribution):\n",
    "                pipe.set_params(**{prm: trial.suggest_float(prm, dist.low, dist.high, log=dist.log)})\n",
    "            elif isinstance(dist, optuna.distributions.IntDistribution):\n",
    "                pipe.set_params(**{prm: trial.suggest_int(prm, dist.low, dist.high)})\n",
    "        p = cv_predict(pipe, X, y)\n",
    "        return average_precision_score(y, p)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "    )\n",
    "    study.optimize(\n",
    "        obj, n_trials=N_TRIALS,\n",
    "        callbacks=[lambda s, t: s.stop() if t.number - s.best_trial.number >= PATIENCE else None],\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    pipe.set_params(**study.best_params)\n",
    "    best = {k.replace(\"model__\", \"\"): v for k, v in study.best_params.items()}\n",
    "    return pipe, best\n",
    "\n",
    "# ───── 8. Variant runner\n",
    "def run_variant(Xd, yd, Xh, yh, cat, num, mdl_name):\n",
    "    pos_w = (yd == 0).sum() / max((yd == 1).sum(), 1)\n",
    "    base, space = build_estimator(mdl_name, pos_w)\n",
    "    pipe, best = tune(build_pipe(base, cat, num), space, Xd, yd)\n",
    "\n",
    "    clf = CalibratedClassifierCV(pipe, method=\"isotonic\", cv=N_SPLITS_TUNE)\n",
    "    clf.fit(Xd, yd)\n",
    "    p = clf.predict_proba(Xh)[:, 1]\n",
    "\n",
    "    ci = ci_boot(yh, p)\n",
    "    thr, f1b = best_f1(yh, p)\n",
    "\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    f1bs = [\n",
    "        f1_score(yh[s], (p[s] >= thr).astype(int))\n",
    "        for s in rng.choice(np.arange(len(yh)), (N_BOOTSTRAP, len(yh)), True)\n",
    "    ]\n",
    "    lo, hi = np.percentile(f1bs, [2.5, 97.5])\n",
    "    ci[\"F1_best\"] = (f1b, lo, hi)\n",
    "    ci[\"F1_thr\"] = (thr, np.nan, np.nan)\n",
    "\n",
    "    flat = {k: v[0] for k, v in ci.items()}\n",
    "    flat.update({f\"{k}_lo\": v[1] for k, v in ci.items()})\n",
    "    flat.update({f\"{k}_hi\": v[2] for k, v in ci.items()})\n",
    "\n",
    "    # F1@0.20\n",
    "    thr20 = 0.20\n",
    "    f1_20 = f1_score(yh, (p >= thr20).astype(int))\n",
    "    f1_20bs = [\n",
    "        f1_score(yh[s], (p[s] >= thr20).astype(int))\n",
    "        for s in rng.choice(np.arange(len(yh)), (N_BOOTSTRAP, len(yh)), True)\n",
    "    ]\n",
    "    lo20, hi20 = np.percentile(f1_20bs, [2.5, 97.5])\n",
    "    flat[\"F1_020\"] = f1_20\n",
    "    flat[\"F1_020_lo\"] = lo20\n",
    "    flat[\"F1_020_hi\"] = hi20\n",
    "\n",
    "    return clf, flat, best, p\n",
    "\n",
    "# ───── 9. Sensitivity loop\n",
    "for mode, prep in MODES.items():\n",
    "    print(f\"\\n=== Mode: {mode} ===\")\n",
    "    # CC のみ RAW、PI/TS は COMPL\n",
    "    path = RAW_PATH if mode == \"CC\" else COMPL_PATH\n",
    "    print(f\"Reading from: {path}\")\n",
    "    df = pd.read_excel(path).set_index(\"INDEX\")\n",
    "    df = prep(df)\n",
    "\n",
    "    if mode == \"TS\":\n",
    "        period_col = \"Period24_25\"\n",
    "        # 存在チェック\n",
    "        if period_col not in df.columns:\n",
    "            print(f\"Warning: '{period_col}' column not found, skipping TS mode.\")\n",
    "            continue\n",
    "        dev_df  = df[df[period_col] == 0]\n",
    "        test_df = df[df[period_col] == 1]\n",
    "        if \"Event\" not in df.columns or test_df[\"Event\"].sum() < 40:\n",
    "            print(\" -> Event<40 or 'Event' missing, skip TS mode.\")\n",
    "            continue\n",
    "    else:\n",
    "        dev_df, test_df = train_test_split(\n",
    "            df, test_size=0.20, stratify=df[\"Event\"], random_state=RANDOM_STATE\n",
    "        )\n",
    "\n",
    "    Xd, yd = dev_df.drop(\"Event\", axis=1), dev_df[\"Event\"].values\n",
    "    Xh, yh = test_df.drop(\"Event\", axis=1), test_df[\"Event\"].values\n",
    "\n",
    "    res, hp = [], []\n",
    "    for vname, (cat_raw, num_raw) in VARIANTS.items():\n",
    "        CAT = [c for c in cat_raw if c in df.columns]\n",
    "        NUM = [n for n in num_raw if n in df.columns]\n",
    "        for mdl in MODELS:\n",
    "            print(f\"  {vname} – {mdl}\")\n",
    "            clf, met, best, p_hold = run_variant(Xd, yd, Xh, yh, CAT, NUM, mdl)\n",
    "            res.append({\"Mode\": mode, \"Variant\": vname, \"Model\": mdl, **met})\n",
    "            hp.append({\"Mode\": mode, \"Variant\": vname, \"Model\": mdl, **best})\n",
    "\n",
    "            # save\n",
    "            with open(OUTDIR / f\"{mode}_{vname}_{mdl}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(clf, f)\n",
    "            pd.DataFrame({\n",
    "                \"Index\": test_df.index,\n",
    "                \"y_true\": yh,\n",
    "                \"y_pred\": p_hold\n",
    "            }).to_csv(OUTDIR / f\"{mode}_{vname}_{mdl}_preds.csv\", index=False)\n",
    "\n",
    "    pd.DataFrame(res).to_excel(\n",
    "        OUTDIR / f\"{mode}_summary_metrics.xlsx\", index=False, float_format=\"%.4f\")\n",
    "    pd.DataFrame(hp).to_excel(\n",
    "        OUTDIR / f\"{mode}_selected_hyperparameters.xlsx\", index=False)\n",
    "\n",
    "print(\"\\n✓ Sensitivity analyses finished →\", OUTDIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f790628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "dca_only_analysis.py - 2025-08-03\n",
    "Script for generating DCA plots and aggregated tables only.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, warnings, pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# ───────── Const\n",
    "# ★ User must specify these paths\n",
    "RESULTS_DIR = Path(r\"C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\results_20250802_0850\")\n",
    "DATA_PATH = Path(r\"C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\completed_data_20250729.xlsx\")\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "MODEL_ORDER = {\"Logistic\": 0, \"XGBoost\": 1, \"LightGBM\": 2}\n",
    "MODEL_ABBREVIATIONS = {\n",
    "    \"Logistic\": \"LR\",\n",
    "    \"XGBoost\": \"XGB\",\n",
    "    \"LightGBM\": \"LGBM\"\n",
    "}\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ───────── Helper\n",
    "def net_benefit(y_true: np.ndarray, y_pred: np.ndarray, thr: float) -> float:\n",
    "    \"\"\"Calculates net benefit for a given threshold.\"\"\"\n",
    "    n = len(y_true)\n",
    "    tp = ((y_pred >= thr) & (y_true == 1)).sum()\n",
    "    fp = ((y_pred >= thr) & (y_true == 0)).sum()\n",
    "    if thr == 1:\n",
    "        return 0.0 # Avoid division by zero\n",
    "    return tp / n - fp / n * (thr / (1 - thr))\n",
    "\n",
    "def calculate_metrics_at_threshold(y_true, y_pred, threshold):\n",
    "    \"\"\"Calculates a set of metrics for a given threshold.\"\"\"\n",
    "    # Handle thresholds that result in no positive predictions\n",
    "    if (y_pred >= threshold).sum() == 0:\n",
    "        return 0, 1, 0, 1, 0 # Sensitivity, PPV, NetBenefit are 0\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred >= threshold).ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    net_b = net_benefit(y_true, y_pred, threshold)\n",
    "    return sensitivity, specificity, ppv, npv, net_b\n",
    "\n",
    "# ───────── 1. モデル & 予測読み込み\n",
    "models_info: List[Dict] = []\n",
    "for pkl in RESULTS_DIR.glob(\"*_hold_model.pkl\"):\n",
    "    stem = pkl.stem\n",
    "    if not stem.endswith(\"_hold_model\"):\n",
    "        continue\n",
    "    base = stem[:-11]\n",
    "    if \"_\" not in base:\n",
    "        continue\n",
    "    variant, model = base.rsplit(\"_\", 1)\n",
    "    csv = pkl.with_name(base + \"_hold_preds.csv\")\n",
    "    if not csv.exists():\n",
    "        continue\n",
    "    dfp = pd.read_csv(csv)\n",
    "    if {\"y_true\", \"y_pred\"}.issubset(dfp.columns):\n",
    "        models_info.append(dict(\n",
    "            variant=variant, model=model,\n",
    "            y_true=dfp.y_true.values,\n",
    "            y_pred=dfp.y_pred.values\n",
    "        ))\n",
    "\n",
    "if not models_info:\n",
    "    raise RuntimeError(\"対象ファイルが見つかりません。\")\n",
    "\n",
    "# 指定順(Logistic→XGB→LGBM)で並べ替え\n",
    "models_info.sort(\n",
    "    key=lambda d: (d[\"variant\"], MODEL_ORDER.get(d[\"model\"], 99))\n",
    ")\n",
    "print(\"✓ Models and predictions loaded successfully.\")\n",
    "\n",
    "# ───────── 2. hold-out prevalence\n",
    "df_all = pd.read_excel(DATA_PATH)\n",
    "_, hold = train_test_split(df_all, test_size=0.2,\n",
    "                           stratify=df_all.Event, random_state=RANDOM_STATE)\n",
    "prev = hold.Event.mean()\n",
    "print(f\"✓ Hold-out prevalence calculated: {prev:.4f}\")\n",
    "\n",
    "# ───────── 3. DCA plot\n",
    "threshs = np.linspace(0.01, 0.5, 50)\n",
    "plt.figure(figsize=(8, 6))\n",
    "for info in models_info:\n",
    "    nb = [net_benefit(info[\"y_true\"], info[\"y_pred\"], t) for t in threshs]\n",
    "    model_abbr = MODEL_ABBREVIATIONS.get(info[\"model\"], info[\"model\"])\n",
    "    plt.plot(threshs, nb, label=f'{info[\"variant\"]}-{model_abbr}')\n",
    "plt.plot(threshs, np.zeros_like(threshs), \"--\", label=\"Treat None\", color=\"black\")\n",
    "plt.plot(threshs, prev - (1 - prev) * (threshs / (1 - threshs)),\n",
    "         \"--\", label=\"Treat All\", color=\"gray\")\n",
    "plt.xlabel(\"Threshold\"); plt.ylabel(\"Net benefit\")\n",
    "plt.title(\"Decision Curve (hold-out)\")\n",
    "plt.xlim(0, 0.5); plt.ylim(-0.15, 0.15)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "dca_plot_path = RESULTS_DIR / \"DCA_AllModels.jpeg\"\n",
    "plt.savefig(dca_plot_path, dpi=300)\n",
    "plt.close()\n",
    "print(f\"✓ DCA plot saved to: {dca_plot_path}\")\n",
    "\n",
    "# ───────── 4. Detailed DCA table\n",
    "print(\"\\n── Generating detailed Decision Threshold Table... ──\")\n",
    "df_rows = []\n",
    "thresholds_for_table = np.linspace(0.001, 1.0, 1000)\n",
    "\n",
    "for info in models_info:\n",
    "    for thr in thresholds_for_table:\n",
    "        sens, spec, ppv, npv, nb = calculate_metrics_at_threshold(\n",
    "            info[\"y_true\"], info[\"y_pred\"], thr\n",
    "        )\n",
    "        df_rows.append({\n",
    "            'Variant': info[\"variant\"],\n",
    "            'Model': info[\"model\"],\n",
    "            'Threshold': thr,\n",
    "            'Sensitivity': sens,\n",
    "            'Specificity': spec,\n",
    "            'PPV': ppv,\n",
    "            'NPV': npv,\n",
    "            'NetBenefit': nb\n",
    "        })\n",
    "df_thr = pd.DataFrame(df_rows)\n",
    "csv_path = RESULTS_DIR / 'Decision_Threshold_Table.csv'\n",
    "df_thr.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Detailed Decision Threshold Table saved to: {csv_path}\")\n",
    "\n",
    "# ───────── 5. Aggregated DCA table\n",
    "print(\"\\n── Aggregating Decision Threshold Table... ──\")\n",
    "df_thr = pd.read_csv(csv_path)\n",
    "\n",
    "bins = np.arange(0, 1.0001, 0.05)\n",
    "labels = [f\"{bins[i]:.2f}-{bins[i+1]:.2f}\" for i in range(len(bins)-1)]\n",
    "\n",
    "df_thr['ThresholdInterval'] = pd.cut(\n",
    "    df_thr['Threshold'],\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    right=True,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "interval_mid = {\n",
    "    label: (float(label.split('-')[0]) + float(label.split('-')[1])) / 2\n",
    "    for label in labels\n",
    "}\n",
    "\n",
    "df_interval = (\n",
    "    df_thr\n",
    "    .groupby(['Variant', 'Model', 'ThresholdInterval'], dropna=False)\n",
    "    .agg({\n",
    "        'Sensitivity': 'mean',\n",
    "        'Specificity': 'mean',\n",
    "        'PPV': 'mean',\n",
    "        'NPV': 'mean',\n",
    "        'NetBenefit': 'mean'\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_interval['RepresentativeThreshold'] = df_interval['ThresholdInterval'].map(interval_mid)\n",
    "\n",
    "cols = [\n",
    "    'Variant', 'Model', 'ThresholdInterval', 'RepresentativeThreshold',\n",
    "    'Sensitivity', 'Specificity', 'PPV', 'NPV', 'NetBenefit'\n",
    "]\n",
    "df_interval = df_interval[cols]\n",
    "\n",
    "out_path = RESULTS_DIR / 'Decision_Threshold_Table_Aggregated.xlsx'\n",
    "df_interval.to_excel(out_path, index=False)\n",
    "print(f\"✓ Aggregated Decision Threshold Table saved to: {out_path}\")\n",
    "\n",
    "print(\"\\nAll DCA-related tasks completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dca79d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating individual SHAP summary plots (hi-res)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tears\\anaconda3\\envs\\py311\\Lib\\site-packages\\shap\\explainers\\_tree.py:2043: UserWarning: [10:34:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  raw = xgb_model.save_raw(raw_format=\"ubj\")\n",
      "c:\\Users\\tears\\anaconda3\\envs\\py311\\Lib\\site-packages\\shap\\explainers\\_tree.py:2043: UserWarning: [10:34:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  raw = xgb_model.save_raw(raw_format=\"ubj\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done.\n",
      "Generating SHAP interaction heatmap (hi-res, ordered features)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tears\\anaconda3\\envs\\py311\\Lib\\site-packages\\shap\\explainers\\_tree.py:2043: UserWarning: [10:34:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  raw = xgb_model.save_raw(raw_format=\"ubj\")\n",
      "c:\\Users\\tears\\anaconda3\\envs\\py311\\Lib\\site-packages\\shap\\explainers\\_tree.py:2043: UserWarning: [10:41:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  raw = xgb_model.save_raw(raw_format=\"ubj\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done.\n",
      "Generating SHAP interaction heatmaps (ALL labels, auto-small, ordered, A–D)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tears\\anaconda3\\envs\\py311\\Lib\\site-packages\\shap\\explainers\\_tree.py:2043: UserWarning: [10:48:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  raw = xgb_model.save_raw(raw_format=\"ubj\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ saved → C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\results_20250802_0850\\SHAP_Interactions_①Pre-op_XGB.jpeg\n",
      "  ✓ saved → C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\results_20250802_0850\\SHAP_Interactions_①Pre-op_LGBM.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tears\\anaconda3\\envs\\py311\\Lib\\site-packages\\shap\\explainers\\_tree.py:2043: UserWarning: [10:55:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  raw = xgb_model.save_raw(raw_format=\"ubj\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ saved → C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\results_20250802_0850\\SHAP_Interactions_②Peri-op+Intra_XGB.jpeg\n",
      "  ✓ saved → C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\results_20250802_0850\\SHAP_Interactions_②Peri-op+Intra_LGBM.jpeg\n",
      "...Done (individual interaction heatmaps — ALL labels).\n",
      "Generating individual SHAP importance bar plots (hi-res)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tears\\anaconda3\\envs\\py311\\Lib\\site-packages\\shap\\explainers\\_tree.py:2043: UserWarning: [11:01:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  raw = xgb_model.save_raw(raw_format=\"ubj\")\n",
      "c:\\Users\\tears\\anaconda3\\envs\\py311\\Lib\\site-packages\\shap\\explainers\\_tree.py:2043: UserWarning: [11:02:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  raw = xgb_model.save_raw(raw_format=\"ubj\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done.\n",
      "Compositing SHAP summary plots (2x3, hi-res metadata)...\n",
      "✓ 2×3 summary saved → C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\results_20250802_0850\\SHAP_Summary_2x3.jpeg\n",
      "Compositing SHAP importance plots (2x3, hi-res metadata)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tears\\anaconda3\\envs\\py311\\Lib\\site-packages\\PIL\\Image.py:3402: DecompressionBombWarning: Image size (95176600 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tears\\anaconda3\\envs\\py311\\Lib\\site-packages\\PIL\\Image.py:3402: DecompressionBombWarning: Image size (95292300 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tears\\anaconda3\\envs\\py311\\Lib\\site-packages\\PIL\\Image.py:3402: DecompressionBombWarning: Image size (95274500 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 2×3 importance saved → C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\results_20250802_0850\\SHAP_Importance_2x3.jpeg\n",
      "\n",
      "All SHAP figures exported in high resolution!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "shap_only_plots_hires.py  – 2025-08-14 (fixed, robust output + indiv. interactions A–D)\n",
    "────────────────────────────────────────\n",
    "RESULTS_DIR に保存されている\n",
    "  *_hold_model.pkl（学習済み CalibratedClassifierCV など）\n",
    "  *_hold_preds.csv  （y_true, y_pred）  # ← 本コードでは未使用\n",
    "を読み込み、\n",
    "\n",
    "  1) SHAP summary plot（各モデルごとに個別 JPEG/TIFF）\n",
    "  2) SHAP interaction heatmap（Pre-op / Peri-op × XGB / LGBM）  ← 2×2総覧\n",
    "  3) SHAP mean(|value|) bar（各モデルごとに個別）\n",
    "  4) 2×3 合成（summary / bar それぞれ）\n",
    "  5) ★ 個別 SHAP interaction（A～Dラベル付き、Appendix向け高解像・余白極小） ← 7b\n",
    "\n",
    "を高解像度（既定 dpi=900）で保存します。\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from PIL import Image, ImageFont\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ───────── 0. 出力品質・見やすさ設定（ここを調整）\n",
    "DPI_EXPORT = 900              # 900〜1200 推奨（雑誌要件に応じて調整）\n",
    "SAVE_TIFF  = False            # True にすると TIFF(LZW) も同時保存\n",
    "FIGSIZE_SUMMARY  = (14, 10)   # SHAP summary 図サイズ\n",
    "FIGSIZE_BAR      = (12, 10)   # SHAP importance bar 図サイズ\n",
    "FIGSIZE_INTERACT = (34, 26)   # 2×2 heatmap 図サイズ（大きめ）\n",
    "INTERACTION_TOP_K = None      # None=全特徴表示（重い場合は 30 などで上位に制限）\n",
    "# パネルラベル（通常SHAP 2×3 用）\n",
    "PANEL_LABELS_2x3 = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
    "\n",
    "# 合成用（巨大画像対策）\n",
    "CELL_MAX_W = 3000            # 各セルの最大横幅（合成前に縮小）\n",
    "MAX_PIXELS_TOTAL = 70_000_000  # 合成後の総ピクセル上限（安全サイズ）\n",
    "\n",
    "# 全体のフォント（sans-serif/大きめ）※ DejaVu Sans を優先（Unicode 安定）\n",
    "matplotlib.rcParams.update({\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": [\"DejaVu Sans\", \"Arial\", \"Helvetica\"],\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 14,\n",
    "    \"ytick.labelsize\": 14,\n",
    "    \"legend.fontsize\": 16,\n",
    "})\n",
    "\n",
    "# SHAP / sklearn の警告抑制（環境差のメッセージ）\n",
    "warnings.filterwarnings(\"ignore\", message=\".*TreeExplainer shap values output has changed.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning,\n",
    "                        message=\"X has feature names, but .* fitted without feature names\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # GPU 不使用（警告低減）\n",
    "\n",
    "# ───────── 1. Un-pickle 用ダミークラス（環境間互換のため）\n",
    "class WinsorizeLog1pTransformer:\n",
    "    def __init__(self, *_, **__): pass\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X): return X\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return (np.asarray(input_features, dtype=object)\n",
    "                if input_features is not None else np.array([]))\n",
    "\n",
    "# 互換名で拾われるケース対策（あなたの環境に合わせて）\n",
    "import builtins\n",
    "builtins.WinsorLog1pTransformer = WinsorizeLog1pTransformer\n",
    "\n",
    "# ───────── 2. パス & 定数（必要に応じて書き換え）\n",
    "RESULTS_DIR = Path(\n",
    "    r\"C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\results_20250802_0850\"\n",
    ")\n",
    "DATA_PATH = Path(\n",
    "    r\"C:\\Users\\tears\\Desktop\\Study\\2025\\03_anesthesiology\\003_ML\\003_Ver3\\completed_data_20250729.xlsx\"\n",
    ")\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "MODEL_ORDER = [\"Logistic\", \"XGBoost\", \"LightGBM\"]\n",
    "VARIANTS = [\"①Pre-op\", \"②Peri-op+Intra\"]\n",
    "VARIANT_DISPLAY_NAMES = {\"①Pre-op\": \"①Pre-op\", \"②Peri-op+Intra\": \"②Peri-op\"}\n",
    "MODEL_ABBREVIATIONS = {\"Logistic\": \"LR\", \"XGBoost\": \"XGB\", \"LightGBM\": \"LGBM\"}\n",
    "\n",
    "# 図面での表示順（存在しないものは自動的にスキップ）\n",
    "ORDERED_FEATURES = [\n",
    "    \"Age\",\"Male\",\"BMI\",\"ASA\",\"Malig\",\"CHF\",\"Dialysis\",\"WBC\",\"Hb\",\"PLT\",\"Alb\",\"CRP\",\n",
    "    \"BUN\",\"Cre\",\"K\",\"Na\",\"T-Bil\",\"DM Med\",\"β-blocker\",\"Anticoag\",\"Antiplatelet\",\n",
    "    \"AntiCa\",\"Opioid\",\"Oral steroids\",\"Proc-Eye\",\"Proc-Face/Neck\",\"Proc-Thorax\",\n",
    "    \"Proc-MSK\",\"Proc-ENT\",\"Proc-Neuro\",\"Proc-Genital\",\"Proc-Urinary\",\"Proc-Skin\",\n",
    "    \"Proc-Abd\",\"ResectNum\",\"HighRiskProc\",\"OpTime\",\"FluidBal\",\"RBC Tx\",\"FFP Tx\",\n",
    "    \"PLT Tx\",\"HR at 6h\",\"MAP at 6h\"\n",
    "]\n",
    "\n",
    "# ───────── 3. ヘルパー\n",
    "def load_pipeline(pkl: Path):\n",
    "    \"\"\"CalibratedClassifierCV を元の estimator に戻す。そうでなければそのまま。\"\"\"\n",
    "    with open(pkl, \"rb\") as f:\n",
    "        mdl = pickle.load(f)\n",
    "    if isinstance(mdl, CalibratedClassifierCV):\n",
    "        return mdl.calibrated_classifiers_[0].estimator\n",
    "    return mdl\n",
    "\n",
    "def get_display_feature_names(original_cols):\n",
    "    \"\"\" 図面表示用の名前正規化 \"\"\"\n",
    "    disp = []\n",
    "    for col in original_cols:\n",
    "        name = col\n",
    "        name = name.replace(\"DeliMed\", \"DM Med\")\n",
    "        name = name.replace(\"ASA\", \"ASA-PS\")\n",
    "        disp.append(name)\n",
    "    return disp\n",
    "\n",
    "def save_with_formats(fig, path_no_ext: Path, dpi=DPI_EXPORT):\n",
    "    \"\"\"JPEG（必要ならTIFFも）で保存。Matplotlib→高品質JPEG.\"\"\"\n",
    "    jpg = path_no_ext.with_suffix(\".jpeg\")\n",
    "    fig.savefig(jpg, dpi=dpi, format=\"jpeg\", bbox_inches=\"tight\")\n",
    "    if SAVE_TIFF:\n",
    "        tiff = path_no_ext.with_suffix(\".tiff\")\n",
    "        fig.savefig(tiff, dpi=dpi, format=\"tiff\",\n",
    "                    pil_kwargs={\"compression\": \"tiff_lzw\"},\n",
    "                    bbox_inches=\"tight\")\n",
    "    return jpg\n",
    "\n",
    "def pil_save_with_dpi(img: Image.Image, path: Path, dpi=DPI_EXPORT, quality=95):\n",
    "    \"\"\"合成画像をPillowで保存（dpiメタ情報・高品質JPEG）。\"\"\"\n",
    "    try:\n",
    "        img.save(path, format=\"JPEG\", quality=quality, subsampling=0, optimize=True, dpi=(dpi, dpi))\n",
    "    except TypeError:\n",
    "        img.save(path, format=\"JPEG\", quality=quality, optimize=True, dpi=(dpi, dpi))\n",
    "    if SAVE_TIFF:\n",
    "        img.save(path.with_suffix(\".tiff\"),\n",
    "                 format=\"TIFF\", compression=\"tiff_lzw\", dpi=(dpi, dpi))\n",
    "\n",
    "def _remove_circled_digits(s: str) -> str:\n",
    "    return re.sub(r\"[①②③④⑤⑥⑦⑧⑨⑩]\", \"\", s).strip()\n",
    "\n",
    "def _safe_resize(img: Image.Image, target_max_pixels=MAX_PIXELS_TOTAL) -> Image.Image:\n",
    "    w, h = img.size\n",
    "    p = w * h\n",
    "    if p <= target_max_pixels:\n",
    "        return img\n",
    "    import math\n",
    "    scale = math.sqrt(target_max_pixels / p)\n",
    "    new_size = (max(1, int(w*scale)), max(1, int(h*scale)))\n",
    "    return img.resize(new_size, Image.LANCZOS)\n",
    "\n",
    "def _load_and_fit_cell(path: Path, cell_max_w=CELL_MAX_W) -> Image.Image:\n",
    "    im = Image.open(path).convert(\"RGB\")\n",
    "    if im.width > cell_max_w:\n",
    "        r = cell_max_w / im.width\n",
    "        im = im.resize((int(im.width*r), int(im.height*r)), Image.LANCZOS)\n",
    "    return im\n",
    "\n",
    "def _compose_grid(image_paths, grid=(2,3), pad=20, bg=(255,255,255)):\n",
    "    \"\"\"安全縮小込みの 2×3 合成。image_paths は左上→右下の順。\"\"\"\n",
    "    from PIL import Image as PILImage\n",
    "    rows, cols = grid\n",
    "    imgs = []\n",
    "    for p in image_paths:\n",
    "        if p and Path(p).exists():\n",
    "            imgs.append(_load_and_fit_cell(Path(p)))\n",
    "        else:\n",
    "            imgs.append(None)\n",
    "\n",
    "    # 列最大幅・行最大高\n",
    "    col_w = [0]*cols\n",
    "    row_h = [0]*rows\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            im = imgs[r*cols + c]\n",
    "            if im:\n",
    "                col_w[c] = max(col_w[c], im.width)\n",
    "                row_h[r] = max(row_h[r], im.height)\n",
    "\n",
    "    W = sum(col_w) + pad*(cols+1)\n",
    "    H = sum(row_h) + pad*(rows+1)\n",
    "    canvas = PILImage.new(\"RGB\", (W, H), bg)\n",
    "\n",
    "    y = pad\n",
    "    for r in range(rows):\n",
    "        x = pad\n",
    "        for c in range(cols):\n",
    "            im = imgs[r*cols + c]\n",
    "            if im:\n",
    "                box = PILImage.new(\"RGB\", (col_w[c], row_h[r]), bg)\n",
    "                ox = (col_w[c]-im.width)//2\n",
    "                oy = (row_h[r]-im.height)//2\n",
    "                box.paste(im, (ox, oy))\n",
    "                canvas.paste(box, (x, y))\n",
    "            x += col_w[c] + pad\n",
    "        y += row_h[r] + pad\n",
    "\n",
    "    return _safe_resize(canvas, target_max_pixels=MAX_PIXELS_TOTAL)\n",
    "\n",
    "# ───────── 4. データ読み込み（hold-out 作成）\n",
    "df_all = pd.read_excel(DATA_PATH)\n",
    "df_all[\"Event\"] = df_all[\"Event\"].astype(int)\n",
    "_, hold = train_test_split(\n",
    "    df_all, test_size=0.2, stratify=df_all[\"Event\"], random_state=RANDOM_STATE\n",
    ")\n",
    "X_hold = hold.drop(columns=[\"Event\"])\n",
    "\n",
    "# ───────── 5. モデルリスト収集\n",
    "models_info = []\n",
    "_pat = re.compile(r\"(.+?)_(.+?)_hold_model\\.pkl$\")   # ex: ①Pre-op_LightGBM_hold_model.pkl\n",
    "for pkl in RESULTS_DIR.glob(\"*_hold_model.pkl\"):\n",
    "    m = _pat.match(pkl.name)\n",
    "    if not m:\n",
    "        continue\n",
    "    variant, model = m.groups()\n",
    "    if variant in VARIANTS and model in MODEL_ORDER:\n",
    "        models_info.append({\"variant\": variant, \"model\": model, \"pkl\": pkl})\n",
    "\n",
    "if not models_info:\n",
    "    raise RuntimeError(\"RESULTS_DIR に対象モデルが見つかりません。ファイル名規則を確認してください。\")\n",
    "\n",
    "# ───────── 6. SHAP summary（個別、高解像度保存）\n",
    "print(\"Generating individual SHAP summary plots (hi-res)...\")\n",
    "summary_paths = []\n",
    "panel_idx = 0  # A〜F 付与用\n",
    "\n",
    "for var in VARIANTS:\n",
    "    display_var = VARIANT_DISPLAY_NAMES.get(var, var)\n",
    "    for mdl in MODEL_ORDER:\n",
    "        info = next((d for d in models_info if d[\"variant\"] == var and d[\"model\"] == mdl), None)\n",
    "        if info is None:\n",
    "            continue\n",
    "\n",
    "        pipe = load_pipeline(info[\"pkl\"])\n",
    "        pre = pipe.named_steps[\"pre\"]\n",
    "        model = pipe.named_steps[\"model\"]\n",
    "\n",
    "        Xp = pre.transform(X_hold)\n",
    "        cols = [c.split(\"__\")[-1] for c in pre.get_feature_names_out()]\n",
    "        dfp  = pd.DataFrame(Xp, columns=cols)\n",
    "        disp_cols = get_display_feature_names(cols)\n",
    "\n",
    "        if mdl == \"Logistic\":\n",
    "            expl = shap.LinearExplainer(model, dfp, feature_perturbation=\"interventional\")\n",
    "            sv = expl.shap_values(dfp)\n",
    "        else:\n",
    "            expl = shap.TreeExplainer(model)\n",
    "            raw = expl.shap_values(dfp)\n",
    "            sv = raw[1] if isinstance(raw, list) else raw\n",
    "\n",
    "        # パネル記号（タイトルは ASCII のみ）\n",
    "        label = chr(65 + panel_idx)  # 'A'=65\n",
    "        clean_var = _remove_circled_digits(display_var)\n",
    "\n",
    "        fig = plt.figure(figsize=FIGSIZE_SUMMARY)\n",
    "        shap.summary_plot(sv, dfp, max_display=20, show=False, feature_names=disp_cols)\n",
    "        plt.title(f\"{label}. {clean_var}-{MODEL_ABBREVIATIONS[mdl]}\", fontsize=18)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        out_no_ext = RESULTS_DIR / f\"SHAP_{display_var}_{MODEL_ABBREVIATIONS[mdl]}\"\n",
    "        jpg_path = save_with_formats(fig, out_no_ext, dpi=DPI_EXPORT)\n",
    "        summary_paths.append(jpg_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "        panel_idx += 1\n",
    "print(\"...Done.\")\n",
    "\n",
    "# ───────── 7. SHAP interaction heatmap（2×2, ORDERED_FEATURES順, 80%閾値マスク：総覧版）\n",
    "print(\"Generating SHAP interaction heatmap (hi-res, ordered features)...\")\n",
    "COMBOS = [(\"①Pre-op\", \"XGBoost\"), (\"①Pre-op\", \"LightGBM\"),\n",
    "          (\"②Peri-op+Intra\", \"XGBoost\"), (\"②Peri-op+Intra\", \"LightGBM\")]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=FIGSIZE_INTERACT)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (var, mdl) in enumerate(COMBOS):\n",
    "    ax = axes[idx]\n",
    "    display_var = VARIANT_DISPLAY_NAMES.get(var, var)\n",
    "    info = next((d for d in models_info if d[\"variant\"] == var and d[\"model\"] == mdl), None)\n",
    "\n",
    "    if info is None:\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"{display_var}-{mdl}\\n(モデル無し)\", fontsize=16)\n",
    "        continue\n",
    "\n",
    "    pipe = load_pipeline(info[\"pkl\"])\n",
    "    pre = pipe.named_steps[\"pre\"]\n",
    "    model = pipe.named_steps[\"model\"]\n",
    "\n",
    "    Xp = pre.transform(X_hold)\n",
    "    cols = [c.split(\"__\")[-1] for c in pre.get_feature_names_out()]\n",
    "    dfp  = pd.DataFrame(Xp, columns=cols)\n",
    "\n",
    "    expl = shap.TreeExplainer(model)\n",
    "    raw = expl.shap_interaction_values(dfp)\n",
    "    mat = raw[1] if isinstance(raw, list) else raw\n",
    "    mat = np.abs(mat).mean(axis=0)\n",
    "    mat_df = pd.DataFrame(mat, index=dfp.columns, columns=dfp.columns)\n",
    "    np.fill_diagonal(mat_df.values, 0)\n",
    "\n",
    "    # ORDERED_FEATURES順に並べ替え（存在する列だけ）\n",
    "    ordered = [f for f in ORDERED_FEATURES if f in mat_df.columns]\n",
    "    mat_df = mat_df.loc[ordered, ordered]\n",
    "\n",
    "    # 80%閾値マスク（弱い相互作用を非表示）\n",
    "    thresh = np.percentile(mat_df.values, 80)\n",
    "    mask = mat_df < thresh\n",
    "\n",
    "    show_cbar = (idx % 2 == 1)\n",
    "    hm = sns.heatmap(\n",
    "        mat_df, mask=mask, cmap=\"viridis\", square=True, ax=ax,\n",
    "        cbar=show_cbar,\n",
    "        cbar_kws={\"label\": \"Mean |SHAP interaction|\"} if show_cbar else None,\n",
    "        linewidths=0.5, linecolor=\"white\"\n",
    "    )\n",
    "    clean_var = _remove_circled_digits(display_var)\n",
    "    ax.set_title(f\"{clean_var}\\n{MODEL_ABBREVIATIONS[mdl]}\", fontsize=16)\n",
    "    ax.set_xticklabels(mat_df.columns, rotation=90, fontsize=10)\n",
    "    ax.set_yticklabels(mat_df.index, rotation=0, fontsize=10)\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "out_no_ext = RESULTS_DIR / \"SHAP_Interactions_2x2\"\n",
    "save_with_formats(fig, out_no_ext, dpi=DPI_EXPORT)\n",
    "plt.close(fig)\n",
    "print(\"...Done.\")\n",
    "\n",
    "# ───────── 7b. SHAP interaction heatmaps（個別・全ラベル表示・小文字自動・ORDERED順・A–D）\n",
    "print(\"Generating SHAP interaction heatmaps (ALL labels, auto-small, ordered, A–D)...\")\n",
    "\n",
    "# 出力パラメータ（必要に応じて調整）\n",
    "INTER_DPI    = 1200          # 高解像 Appendix 用\n",
    "CELL_IN      = 0.16          # 1セルの物理サイズ（インチ）。列数×CELL_IN が横幅になる\n",
    "MIN_FIG_IN   = 3.5           # 図の最小サイズ（インチ）安全策\n",
    "MASK_PERCENT = 80            # 弱い相互作用のマスク％（全セル表示なら None）\n",
    "\n",
    "# A〜D の順序\n",
    "COMBOS = [\n",
    "    (\"①Pre-op\",        \"XGBoost\"),   # A\n",
    "    (\"①Pre-op\",        \"LightGBM\"),  # B\n",
    "    (\"②Peri-op+Intra\", \"XGBoost\"),   # C\n",
    "    (\"②Peri-op+Intra\", \"LightGBM\"),  # D\n",
    "]\n",
    "PANEL_LABELS = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "from matplotlib import ticker\n",
    "\n",
    "def _font_sizes_from_cell(cell_in):\n",
    "    \"\"\"セルの物理サイズ（インチ）からフォント pt を自動算出（上下限あり）。\"\"\"\n",
    "    tick_pt  = int(max(4,  min(9,  cell_in * 72 * 0.70)))  # 目盛ラベル\n",
    "    title_pt = int(max(8,  min(14, cell_in * 72 * 1.10)))  # タイトル\n",
    "    cbar_pt  = int(max(4,  min(10, cell_in * 72 * 0.63)))  # カラーバー\n",
    "    return tick_pt, title_pt, cbar_pt\n",
    "\n",
    "def _figsize_from_cells_in(n_rows, n_cols, cell_in=CELL_IN):\n",
    "    \"\"\"セル数×セル物理サイズ から図のインチサイズを決定。\"\"\"\n",
    "    w_in = max(MIN_FIG_IN, n_cols * cell_in)\n",
    "    h_in = max(MIN_FIG_IN, n_rows * cell_in)\n",
    "    return (w_in, h_in)\n",
    "\n",
    "def _compute_interaction_df_for(pipe, X_hold):\n",
    "    \"\"\"平均|SHAP相互作用|行列を作成し、表示名へ正規化＋ORDERED_FEATURES順に整列。\"\"\"\n",
    "    pre = pipe.named_steps[\"pre\"]; model = pipe.named_steps[\"model\"]\n",
    "    Xp = pre.transform(X_hold)\n",
    "    cols = [c.split(\"__\")[-1] for c in pre.get_feature_names_out()]\n",
    "    # 表示名へ正規化\n",
    "    disp_cols = [c.replace(\"DeliMed\",\"DM Med\").replace(\"ASA\",\"ASA-PS\") for c in cols]\n",
    "    dfp  = pd.DataFrame(Xp, columns=disp_cols)\n",
    "\n",
    "    expl = shap.TreeExplainer(model)\n",
    "    raw  = expl.shap_interaction_values(dfp)\n",
    "    mat  = raw[1] if isinstance(raw, list) else raw       # (n_samples, n_feat, n_feat)\n",
    "    mat  = np.abs(mat).mean(axis=0)                       # 平均 |interaction|\n",
    "\n",
    "    mat_df = pd.DataFrame(mat, index=dfp.columns, columns=dfp.columns)\n",
    "    np.fill_diagonal(mat_df.values, 0)\n",
    "\n",
    "    # ORDERED_FEATURES で並べ替え（存在する列のみ）\n",
    "    ordered = [f for f in ORDERED_FEATURES if f in mat_df.columns]\n",
    "    return mat_df.loc[ordered, ordered] if ordered else mat_df\n",
    "\n",
    "for idx, (var, mdl) in enumerate(COMBOS):\n",
    "    info = next((d for d in models_info if d[\"variant\"] == var and d[\"model\"] == mdl), None)\n",
    "    if info is None:\n",
    "        print(f\"  [skip] {var}-{mdl}: モデル無し\"); continue\n",
    "\n",
    "    pipe   = load_pipeline(info[\"pkl\"])\n",
    "    mat_df = _compute_interaction_df_for(pipe, X_hold)\n",
    "    n_rows, n_cols = mat_df.shape\n",
    "    if n_rows == 0 or n_cols == 0:\n",
    "        print(f\"  [skip] {var}-{mdl}: 空の行列\"); continue\n",
    "\n",
    "    # マスク（※全セル表示したい場合は MASK_PERCENT=None に）\n",
    "    mask = None\n",
    "    if MASK_PERCENT is not None:\n",
    "        thresh = np.percentile(mat_df.values, MASK_PERCENT)\n",
    "        mask   = mat_df < thresh\n",
    "\n",
    "    # 図サイズ（インチ）とフォント（pt）をセル寸法から決定\n",
    "    figsize = _figsize_from_cells_in(n_rows, n_cols, CELL_IN)\n",
    "    tick_pt, title_pt, cbar_pt = _font_sizes_from_cell(CELL_IN)\n",
    "\n",
    "    # 描画（constrained_layout は使わない：文字巨大化を避ける）\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    hm = sns.heatmap(\n",
    "        mat_df, mask=mask, cmap=\"viridis\", square=True, ax=ax,\n",
    "        cbar=True, cbar_kws={\"label\": \"Mean |SHAP interaction|\"},\n",
    "        linewidths=0.35, linecolor=\"white\"\n",
    "    )\n",
    "\n",
    "    # 目盛り位置はセル中心に固定 → ラベル数不一致エラーを防止\n",
    "    ax.xaxis.set_major_locator(ticker.FixedLocator(np.arange(n_cols) + 0.5))\n",
    "    ax.yaxis.set_major_locator(ticker.FixedLocator(np.arange(n_rows) + 0.5))\n",
    "\n",
    "    # ★ 全ラベル表示（間引きなし）／小フォント\n",
    "    ax.set_xticklabels(list(mat_df.columns), rotation=90, fontsize=tick_pt)\n",
    "    ax.set_yticklabels(list(mat_df.index),   rotation=0,  fontsize=tick_pt)\n",
    "    ax.tick_params(axis=\"x\", length=0, pad=1)\n",
    "    ax.tick_params(axis=\"y\", length=0, pad=1)\n",
    "\n",
    "    # タイトル：A–D 付与、①②除去、C/D は \"+Intra\" を消す（VARIANT_DISPLAY_NAMES を尊重）\n",
    "    display_var = VARIANT_DISPLAY_NAMES.get(var, var)     # ②Peri-op+Intra → ②Peri-op\n",
    "    clean_var   = _remove_circled_digits(display_var).replace(\"+Intra\", \"\").strip()\n",
    "    mdl_abbr    = MODEL_ABBREVIATIONS.get(mdl, mdl)\n",
    "    ax.set_title(f\"{PANEL_LABELS[idx]}. {clean_var} — {mdl_abbr}\", fontsize=title_pt, pad=3)\n",
    "\n",
    "    # カラーバーの文字も小さく\n",
    "    if hm and len(hm.collections) > 0:\n",
    "        cbar = hm.collections[0].colorbar\n",
    "        cbar.ax.tick_params(labelsize=cbar_pt)\n",
    "        cbar.set_label(\"Mean |SHAP interaction|\", fontsize=cbar_pt)\n",
    "\n",
    "    # 保存\n",
    "    out_stub = RESULTS_DIR / f\"SHAP_Interactions_{var}_{mdl_abbr}\"\n",
    "    fig.savefig(out_stub.with_suffix(\".jpeg\"),\n",
    "                dpi=INTER_DPI, bbox_inches=\"tight\", pad_inches=0.02, format=\"jpeg\")\n",
    "    if SAVE_TIFF:\n",
    "        fig.savefig(out_stub.with_suffix(\".tiff\"),\n",
    "                    dpi=INTER_DPI, bbox_inches=\"tight\", pad_inches=0.02,\n",
    "                    format=\"tiff\", pil_kwargs={\"compression\": \"tiff_lzw\"})\n",
    "    plt.close(fig)\n",
    "    print(f\"  ✓ saved → {out_stub.with_suffix('.jpeg')}\")\n",
    "\n",
    "print(\"...Done (individual interaction heatmaps — ALL labels).\")\n",
    "\n",
    "\n",
    "# ───────── 8. SHAP mean(|value|) bar（個別、高解像度保存）\n",
    "print(\"Generating individual SHAP importance bar plots (hi-res)...\")\n",
    "imp_paths = []\n",
    "for var in VARIANTS:\n",
    "    display_var = VARIANT_DISPLAY_NAMES.get(var, var)\n",
    "    for mdl in MODEL_ORDER:\n",
    "        info = next((d for d in models_info if d[\"variant\"] == var and d[\"model\"] == mdl), None)\n",
    "        if info is None:\n",
    "            continue\n",
    "\n",
    "        pipe = load_pipeline(info[\"pkl\"])\n",
    "        pre = pipe.named_steps[\"pre\"]\n",
    "        model = pipe.named_steps[\"model\"]\n",
    "\n",
    "        Xp = pre.transform(X_hold)\n",
    "        cols = [c.split(\"__\")[-1] for c in pre.get_feature_names_out()]\n",
    "        dfp  = pd.DataFrame(Xp, columns=cols)\n",
    "\n",
    "        if mdl == \"Logistic\":\n",
    "            expl = shap.LinearExplainer(model, dfp, feature_perturbation=\"interventional\")\n",
    "            sv = expl.shap_values(dfp)\n",
    "        else:\n",
    "            expl = shap.TreeExplainer(model)\n",
    "            raw = expl.shap_values(dfp)\n",
    "            sv = raw[1] if isinstance(raw, list) else raw\n",
    "\n",
    "        imp = (pd.Series(np.abs(sv).mean(axis=0), index=dfp.columns)\n",
    "                 .rename(index={\"DeliMed\": \"DM Med\"})\n",
    "                 .sort_values(ascending=False)\n",
    "                 .head(20))\n",
    "\n",
    "        fig = plt.figure(figsize=FIGSIZE_BAR)\n",
    "        sns.barplot(x=imp.values, y=imp.index, orient=\"h\", palette=\"viridis\")\n",
    "        plt.xlabel(\"mean(|SHAP value|)\", fontsize=16)\n",
    "        plt.ylabel(\"\")\n",
    "        plt.title(f\"{display_var}-{MODEL_ABBREVIATIONS[mdl]}  Feature importance (Top 20)\", fontsize=18)\n",
    "        plt.tight_layout()\n",
    "        out_no_ext = RESULTS_DIR / f\"SHAP_Importance_{display_var}_{MODEL_ABBREVIATIONS[mdl]}\"\n",
    "        jpg_path = save_with_formats(fig, out_no_ext, dpi=DPI_EXPORT)\n",
    "        imp_paths.append(jpg_path)\n",
    "        plt.close(fig)\n",
    "print(\"...Done.\")\n",
    "\n",
    "# ───────── 9. 合成（summary 2×3、bar 2×3）— Pillow で dpi メタ情報も付与（安全縮小あり）\n",
    "print(\"Compositing SHAP summary plots (2x3, hi-res metadata)...\")\n",
    "summary_grid = []\n",
    "for v in VARIANTS:\n",
    "    display_v = VARIANT_DISPLAY_NAMES.get(v, v)\n",
    "    for m in MODEL_ORDER:\n",
    "        p = RESULTS_DIR / f\"SHAP_{display_v}_{MODEL_ABBREVIATIONS[m]}.jpeg\"\n",
    "        if p.exists():\n",
    "            summary_grid.append(p)\n",
    "        else:\n",
    "            raise FileNotFoundError(p)\n",
    "\n",
    "summary_canvas = _compose_grid(summary_grid, grid=(2,3), pad=20, bg=(255,255,255))\n",
    "out_path = RESULTS_DIR / \"SHAP_Summary_2x3.jpeg\"\n",
    "pil_save_with_dpi(summary_canvas, out_path, dpi=DPI_EXPORT, quality=95)\n",
    "print(f\"✓ 2×3 summary saved → {out_path}\")\n",
    "\n",
    "print(\"Compositing SHAP importance plots (2x3, hi-res metadata)...\")\n",
    "imp_grid = []\n",
    "for v in VARIANTS:\n",
    "    display_v = VARIANT_DISPLAY_NAMES.get(v, v)\n",
    "    for m in MODEL_ORDER:\n",
    "        p = RESULTS_DIR / f\"SHAP_Importance_{display_v}_{MODEL_ABBREVIATIONS[m]}.jpeg\"\n",
    "        if p.exists():\n",
    "            imp_grid.append(p)\n",
    "        else:\n",
    "            raise FileNotFoundError(p)\n",
    "\n",
    "importance_canvas = _compose_grid(imp_grid, grid=(2,3), pad=20, bg=(255,255,255))\n",
    "out_path2 = RESULTS_DIR / \"SHAP_Importance_2x3.jpeg\"\n",
    "pil_save_with_dpi(importance_canvas, out_path2, dpi=DPI_EXPORT, quality=95)\n",
    "print(f\"✓ 2×3 importance saved → {out_path2}\")\n",
    "\n",
    "print(\"\\nAll SHAP figures exported in high resolution!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
